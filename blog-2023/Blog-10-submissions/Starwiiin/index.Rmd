---
author: "Parvin Mohammadiarvejeh"
title: "Web scraping etiquette ..."
categories: "Errors and warnings in packages"
date: "2023-04-06"
output: github_document
---
 
## Prompt:

With great power comes great responsibility - a large part of the web is based on data and services that scrape those data. 
Now that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.

Find sources on ethical web scraping - some readings that might help you get started with that are: 

  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)

  - R package [polite](https://github.com/dmi3kno/polite)

  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)


After reading through some of the ethics essays
write a blog post addressing the following questions: 

1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**
Parvin's answer: 1) I will always mention and clarify my intentions by using the data that I scarp and I will not scrap the data that I do not need. For example, suppose that I scrapped data from a web, if I am analyzing the relationship between two variables including exercise and lifetime, I will mention my goal in my github and also I will not scrap the data related to the diet which I do not use it in my analysis. 2) I will always try to give the credit back to the owner of the website. For example I will mention it in the paper as the reference. 3) I will always extract data from a web to create value, new, and useful information. In the other words, I will not copy the data to put it my webpage. For example, I will use the data to run a prediction model to publish insights.
2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**
Parvin's answer: robots.txt file which is used in the "polite" package and it should be called in the process of the web scarping to introduce the client (who will use from the data by web scarping) to the host and get the permission to scrape. Also, it helps to take the information with slower rate and the client needs to ask the permission just one time. 
3. **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**

```{r}
library(polite)
library(rvest)
library(purrr)
library(dplyr)


session = bow("https://www.nytimes.com/elections/2016/results/iowa") 
html = scrape(session)
html




```




```{r}
tables <- html %>% html_table(fill=TRUE)
tables %>% purrr::map(glimpse)
```








Instructions:

Submit to your repo. Make sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)

