---
author: "Muxin Hua"
title: "Web scraping etiquette ..."
categories: "Errors and warnings in packages"
date: "2023-04-06"
output: github_document
---

## Prompt:

With great power comes great responsibility - a large part of the web is based on data and services that scrape those data. Now that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.

Find sources on ethical web scraping - some readings that might help you get started with that are:

-   [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)

-   R package [polite](https://github.com/dmi3kno/polite)

-   [JAMI \@ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)

After reading through some of the ethics essays write a blog post addressing the following questions:

1.  **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**

My takeaways are from both side: being respectful and grateful when scraping, and being open to ethical scrapers when being an owner.

To me, scraping is like making a cold call: it takes two to make the deal. Visitors should leave the customer alone if there’s a “No soliciting”. If no such a sign, visitors need to knock on the door before getting in. After getting in, visitors are responsible for identifying themselves, following the instructions from the owner, being polite, and saying thanks before leaving. These correspond to access APIs if there’s any, reasonably request data, respect rules and data, showing gratitude.

On the other hand, the owner can make rules or signs to avoid confusion. If the owner decides to start a conversation, respecting the visitors' follow rules, explain why he needs the visitors to leave if there’s any situation. These correspond to considering public APIs, allowing ethical scrapers, and reaching out to scrapers before blocking.

1.  **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**

    A robots.txt file tells scrappers which URLs can be accessed. Here's a line of `robots.txt`.

    > User-agent: Googlebot
    >
    > Disallow: /nogooglebot/

The user agent named Googlebot is not allowed to crawl any URL that starts with <https://example.com/nogooglebot/>.

> User-agent: \*
>
> Allow: /

Above lines mean all user agent are allowed to crawl the entire site.

**Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**
```{r}
library(polite)
library(rvest)
library(dplyr)

url <- "https://en.wikipedia.org/wiki/AFC_Asian_Cup_records_and_statistics"
session <- bow(url, user_agent = "hiiiua blog-10 assignment")

section <- scrape(session) %>% html_nodes("#mw-content-text > div.mw-parser-output > table:nth-child(5)")

section %>% html_table()

```


Instructions:

Submit to your repo. Make sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)
