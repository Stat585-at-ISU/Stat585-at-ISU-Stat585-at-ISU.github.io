---
author: "Harun Celik"
title: "Split-Apply-Combine Post"
categories: Split-apply-combine
date: 2023-02-16
output: github_document
editor_options: 
  markdown: 
    wrap: 72
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Prompt:

The `plyr` package has by now been replaced by other, even faster
packages, but the idea of *Split, apply, combine* is as relevant as
ever.

Read the paper [The Split-Apply-Combine Strategy for Data
Analysis](https://www.jstatsoft.org/article/view/v040i01) by Hadley
Wickham.

Write a blog post addressing the following questions:

1.  The R code for the split-apply-combine paper is posted with the
    paper. Pick one of the examples demonstrating `plyr` functionality
    (such as `dlply` or `ddply`, ...) and rewrite the example using
    functionality from the package `dplyr`. Make sure that your example
    works and the results are identical.

2.  Which (base R) functions do you know that support the
    split-apply-combine strategy? In your opinion, are these sufficient?
    State why or why not?

3.  The ChatGPT response to the prompt "Describe the split-apply-combine
    paradigm" is given below. Generally, the chatbot's answers are
    decent. ChatGPT does have problems getting details right. Which
    details would you change in the response?

> The "split-apply-combine" paradigm is a data analysis strategy used in
> statistics and data science to analyze grouped data. It consists of
> three steps:
>
> Split: The data is first split into groups based on one or more
> variables of interest.
>
> Apply: A specific operation or function is applied to each group of
> data, usually for the purpose of aggregating, summarizing, or
> transforming the data within each group.
>
> Combine: The results of the operation applied to each group are then
> combined and returned as a single output.
>
> The split-apply-combine paradigm is particularly useful in cases where
> the data cannot be analyzed as a whole and instead must be analyzed in
> smaller, more manageable chunks. This allows for more efficient data
> processing and improved scalability. The paradigm is commonly
> implemented using tools such as the "groupby" function in the R and
> Python programming languages, and the "dplyr" library in R.

You can write your answers directly the `README.Rmd` file. Make sure
that the file knits (you will need to activate any packages your use in
your code chunks with a call to `library(xxx)`, where xxx is the name of
the package, such as `plyr` ). Commit your changes and push to your
repo; add any files in the `README_files` directory to your repository.

#Answers

## Question One

### Code Selection

I wanted to work with a simple example from Hadley Wickham's code so
that I could better understand in detail what was happening. For this
reason, I chose to work with a chunk of code that used `ddply` to first
group_by the column `$id`, and then to create a new column called
`$cyear` using a base R `transform()` function.

### Here is the original code in `plyr`:

```{r}
library("ggplot2")
library("plyr")
options(digits = 3)
options(prompt = "R> ")
```

```{r}
#Changed variable name to baseball_plyr
baseball_plyr <- ddply(baseball, .(id), transform, 
  cyear = year - min(year) + 1)
head(baseball_plyr)
```

Here is the output replicated in `dplyr`:

```{r load dplyr, message=FALSE}
library(dplyr)
```

```{r replicate in dplyr}
baseball_dplyr <- baseball %>%
  group_by(id) %>%
  mutate(cyear = year - min(year) + 1)
head(baseball_dplyr)
```

### Discussion

In both cases, the input and output is a data frame. While the code is
less intuitive to understand at first glance in `ddply` the input/output
formats are represented easier through the `dd` notation. Both codes
group by `$id` and create a new column that is called `$cyear` with the
cumulative year of a player's career. The `dplyr` package makes use of
the pipeline operator `%>%` to streamline the process of
split-apply-combine whereas `ddply` looks more similar to how the
`apply` functions take care of this paradigm in base r. One interesting
difference that I never paid attention to before was that the `str()`
function produces different results because of the `group_by()` function
in `dplyr`. It looks like the `group_by()` function creates nested
structures when calling `str()` on the data frame.

## Question Two

### Discussion

Until recently, I wasn't following the differences between `base r` and
the `tidyverse` so the only base function I am aware of that follows
this split-apply-combine paradigm are the `apply` functions.

```{r}
by(baseball$g, baseball$id, mean) |>
  head()
```

The code above uses `by` which serves as a wrapper for `tapply` and
while it does calculate the mean of `$g` by `$id`, the information of
the output is not as detailed as when run with the `dplyr` functions. I
also found it difficult to work with the syntax of the new base pipe
`|>` operator despite it serving a similar functionality to the more
widely utilized `magrittr` pipe operator.

## Question Three

Throughout the article, Hadley Wickham demonstrates the importance of
working with the split-apply-combine paradigm efficiently. This is
something that ChatGPT did not pick up on in their answer. The answer
from the AI for instance doesn't mention that in the "split" section,
memory-saving methods, like the ones introduced by `plyr`, are an
important process of working through the paradigm. The AI also doesn't
discuss the limitations of the split-apply-combine method. An important
limitation to the paradigm, especially through use of `plyr` is that
"each piece of data [is] processed only once and independently of all
other pieces" (Wickham, 2011). This is an important element of the
paradigm that separates it from advanced looping functions.

I have to admit that ChatGPT's answer is surprisingly concise even if it
is a little generalized. It would be interesting to see how it answers
more particular questions about the paradigm like, "what are the
limitations of the split-apply-combine paradigm?"
