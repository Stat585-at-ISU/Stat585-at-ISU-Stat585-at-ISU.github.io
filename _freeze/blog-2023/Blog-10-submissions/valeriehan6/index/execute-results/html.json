{
  "hash": "ff77e0e99b7ab476c35bd4bc98717e7d",
  "result": {
    "markdown": "---\nauthor: \"Who wrote this\"\ntitle: \"Web scraping etiquette ...\"\ncategories: \"Web scraping etiquette ...\"\ndate: \"2023-04-06\"\noutput: github_document\n---\n\n \n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data. \nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are: \n\n  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n  - R package [polite](https://github.com/dmi3kno/polite)\n\n  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\nAfter reading through some of the ethics essays\nwrite a blog post addressing the following questions: \n\n1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n\nMy main takeaways are [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) and [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping). In general, try to be kind to one another. Don't harm the website you're scraping. 1) Respect the website's guidelines: use a public API instead if available, respect the `robots.txt` file if the website has one, and respect the terms and conditions of the site. 2) Give credit where it's due: if you scrape data and write something about it, make sure to credit the source of the data. 3) Don't overload the site: space out requests and try to send requests during off-peak hours.\n\n\n\n2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\n\nA `robots.txt` file tells the web-crawling software where it is allowed to go on the website ([JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)). For example, websites can use it to tell search engines like Google which pages should be crawled to understand the website and what search results it should appear in [Introduction to robots.txt](https://developers.google.com/search/docs/crawling-indexing/robots/intro). Note that while the `robots.txt file` tells web crawlers where they are allowed to go, they don't actually enforce those rules themselves.\n\n\n3. **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\n\nI scraped from https://www.nytimes.com/elections/2016/results/massachusetts.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_298cb009b97b074e6bf0629fbbb9c744'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_454c666ff3438b369ca5b287d6507d73'}\n\n```{.r .cell-code}\n# similar to example from class\nurl <- \"https://www.nytimes.com/elections/2016/results/massachusetts\"\nsession <- bow(url)\nhtml <- scrape(session)\ntables <- html %>% html_table()\n\n# tables %>% purrr::map(.f = pillar::glimpse)\n\nma_results <- tables[[2]] %>% dplyr::mutate(\n  Trump  = readr::parse_number(Trump),\n  Clinton = readr::parse_number(Clinton)\n)\nma_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 351 × 3\n   `Vote by town` Clinton Trump\n   <chr>            <dbl> <dbl>\n 1 Boston          221093 38087\n 2 Worcester        43084 17732\n 3 Springfield      40341 11231\n 4 Cambridge        46563  3323\n 5 Newton           36463  7764\n 6 Quincy           25477 13321\n 7 Somerville       33740  4128\n 8 Lowell           23555 10584\n 9 Brockton         25593  8801\n10 Lynn             22164  9311\n# … with 341 more rows\n```\n:::\n:::\n\n\n\nInstructions:\n\nSubmit to your repo. Make sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}