{
  "hash": "9050e9c3e810f937b1ca70d70277a79d",
  "result": {
    "markdown": "---\nauthor: \"Charch\"\ntitle: \"Web scraping\"\ncategories: \"Web scraping etiquette ...\"\ndate: \"2023-04-06\"\noutput: github_document\n---\n\n\nThe json schema for this blog is different in the announcement than the schema on the github. Let me know if you want me to change it.\n \n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data. \nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are: \n\n  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n  - R package [polite](https://github.com/dmi3kno/polite)\n\n  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\nAfter reading through some of the ethics essays\nwrite a blog post addressing the following questions: \n\n1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n\nMy main takeaways from the ethical web scraping is that it takes resources to build and maintain website and we should respect the robot.txt file and their terms of services.\n\nBow and scrape: Basically we should also obtain consent if we are going to use their content and give them the credit.\n\nIt would be nice to provide some value in return if we can. Also, we should respect the data privacy and try to use only the content which is necessary for the work or hobby we are doing. Anything else is best to be not stored on our computer.\n\n2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\n\nRobot.txt file is use to instruct the web robot how to crawl (find stuff on) the web. It communicate what parts of the site are accessible and what actions are allowed. It is usually at the root of a website.\n\nBelow is the example from gooogle search central:\n\n\"\nUser-agent: Googlebot\nDisallow: /nogooglebot/\n\nUser-agent: *\nAllow: /\n\nSitemap: https://www.example.com/sitemap.xml\n\"\n\nThe above file is stopping an agent named \"Googlebot\" to crawl any URL that starts with xyz.com/nogooglebot/ but everyone else is allowed.\n\nRobots.txt is usually a guideline and some users do not seem to follow that online.\n\n\n3. **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\n\nFirst example is just from the second link of this rmd file as a first try for this blog.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_fff100c574dbdf54d6dcd656299a79e3'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\nsession <- bow(\"https://www.cheese.com/by_type\", force = TRUE)\nresult <- scrape(session, query=list(t=\"semi-soft\", per_page=100)) %>%\n  html_node(\"#main-body\") %>% \n  html_nodes(\"h3\") %>% \n  html_text()\nhead(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"3-Cheese Italian Blend\"  \"Abbaye de Citeaux\"      \n[3] \"Abbaye du Mont des Cats\" \"Adelost\"                \n[5] \"ADL Brick Cheese\"        \"Ailsa Craig\"            \n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_f336d7277a3e82cadd74c4082379e5e5'}\n\n```{.r .cell-code}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'janitor'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n:::\n\n```{.r .cell-code}\nsession <- bow(\"https://en.wikipedia.org/wiki/List_of_national_independence_days\", force = TRUE)\nind_html <-\n  polite::scrape(session) %>%\n  rvest::html_nodes(\"table.wikitable\") %>% \n  rvest::html_table(fill = TRUE)\n\nind_tab <- \n  ind_html[[1]] %>% \n  clean_names()\nind_tab\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 202 × 6\n   country       name_of_holiday date_of_holiday year_of_event independence_from\n   <chr>         <chr>           <chr>           <chr>         <chr>            \n 1 Afghanistan   Afghan Indepen… 19 August       1919          United Kingdom   \n 2 Albania       Flag Day (Dita… 28 November     1912          Ottoman Empire   \n 3 Algeria       Independence D… 5 July          1962          France           \n 4 Angola        Independence D… 11 November     1975          Portugal         \n 5 Antigua and … Independence D… 1 November      1981          United Kingdom   \n 6 Argentina     Independence D… 9 July          1816[8]       Spanish Empire   \n 7 Armenia       Republic Day    28 May          1918[9]       Russian Soviet F…\n 8 Armenia       Independence D… 21 September    1991          Soviet Union     \n 9 Azerbaijan    Independence D… 28 May          1918          Russian Soviet F…\n10 Azerbaijan    Independendenc… 18 October      1991[10]      Soviet Union     \n# ℹ 192 more rows\n# ℹ 1 more variable: event_commemorated_and_notes <chr>\n```\n:::\n:::\n\n\nInstructions:\n\nSubmit to your repo. Make sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}