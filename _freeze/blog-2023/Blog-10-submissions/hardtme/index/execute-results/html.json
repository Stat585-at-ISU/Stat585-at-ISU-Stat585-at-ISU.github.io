{
  "hash": "7f1d131d7b4f1a18ff4a1ff559bbebc2",
  "result": {
    "markdown": "---\nauthor: \"Marie Hardt\"\ntitle: \"Web Scraping Etiquette ...\"\ncategories: \"Web scraping etiquette ...\"\ndate: \"2023-04-06\"\noutput: github_document\n---\n\n \n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data. \nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are: \n\n  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n  - R package [polite](https://github.com/dmi3kno/polite)\n\n  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\nAfter reading through some of the ethics essays\nwrite a blog post addressing the following questions: \n\n1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n\nMy three main ethical web scraping takeaways are:\n\n* If a website has an API, you should collect data via the API instead of scraping the website ([James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) and [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)).\n\n* When scraping a website, you should scrape at a rate that doesn't overload the website's host servers or get you confused for a DDoS attack on the website ([James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) and [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)).\n\n* You should only scrape the data that you need and then cite/link back to the website from which you scraped the data ([James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) and [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)).\n\n2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\n\nA ROBOTS.TXT file tells web scrapers what parts of a website they are allowed to access, and can disallow access to some (or all) web scrapers altogether. The vignette on [polite package website](https://dmi3kno.github.io/polite/) links to the ROBOTS.TXT file for Bob Rudis's blog. This [file](https://rud.is/robots.txt) includes a crawl delay that ethical web scrapers will observe along with some impressive text art images of a Star Trek space station. It seems that all scraping of the blog is allowed as long as the crawl delay is observed. Conversely, [Wikipedia](https://en.wikipedia.org/robots.txt) has an extensive ROBOTS.TXT file that disallows many different web crawlers for various reasons, including making too many requests in too short a period of time, being associated with advertising, and more. \n\n3. **Identify a website that you would like to scrape (or an example from class) and implement a scrape using the `polite` package.**\n\nI will revisit an example from class, but implement the web scrape using the `polite` package instead of the `rvest` package. I will scrape the *New York Times* website to acquire the 2016 election results from Oregon. I chose Oregon because I spent the summer of 2019 participating in a research experience for undergraduates (REU) program at Oregon State University in Corvallis. I will then compare the results to those obtained when we scrape the website using the `rvest` package.\n\nFirst, let's scrape the Oregon election results using the `polite` package.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_40bf224a16744feb619d156629a281e8'}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(polite)\nlibrary(rvest)\n\nor_url <- \"https://www.nytimes.com/elections/2016/results/oregon\"\npolite_session <- bow(or_url, force = T)\nor_scrape <- scrape(polite_session)\n\nor_tables <- or_scrape %>% html_table(fill=TRUE)\nor_tables[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 36 × 3\n   `Vote by county` Clinton Trump \n   <chr>            <chr>   <chr> \n 1 Multnomah        292,561 67,954\n 2 Washington       153,251 83,197\n 3 Clackamas        102,095 88,392\n 4 Lane             102,753 67,141\n 5 Marion           57,788  63,377\n 6 Jackson          44,447  53,870\n 7 Deschutes        42,444  45,692\n 8 Linn             17,995  33,488\n 9 Douglas          14,096  34,582\n10 Benton           29,193  13,445\n# ℹ 26 more rows\n```\n:::\n:::\n\n\nNow let's compare the results from the `polite` package to those obtained using the `rvest` package.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_e0092282b297f87fb4e3000f7474b738'}\n\n```{.r .cell-code}\nor_html <- read_html(or_url)\nor_tables2 <- or_html %>% html_table(fill=TRUE)\nor_tables2[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 36 × 3\n   `Vote by county` Clinton Trump \n   <chr>            <chr>   <chr> \n 1 Multnomah        292,561 67,954\n 2 Washington       153,251 83,197\n 3 Clackamas        102,095 88,392\n 4 Lane             102,753 67,141\n 5 Marion           57,788  63,377\n 6 Jackson          44,447  53,870\n 7 Deschutes        42,444  45,692\n 8 Linn             17,995  33,488\n 9 Douglas          14,096  34,582\n10 Benton           29,193  13,445\n# ℹ 26 more rows\n```\n:::\n\n```{.r .cell-code}\nall.equal(or_tables[[2]], or_tables2[[2]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nThe results from both scraping methods are the same.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}