{
  "hash": "2399c02f8d304edc12971d30b9948760",
  "result": {
    "markdown": "---\nauthor: \"Sabrena Rutledge\"\ntitle: \"Web scraping etiquette ...\"\ncategories: \"Errors and warnings in packages\"\ndate: \"2023-04-06\"\noutput: github_document\n---\n\n \n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data. \nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are: \n\n  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n  - R package [polite](https://github.com/dmi3kno/polite)\n\n  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\nAfter reading through some of the ethics essays\nwrite a blog post addressing the following questions: \n\n1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\nBe considerate to the page that you are web-scraping: cite the author(s) and do not claim their data as your own. (https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\nIf there is an API on the website for its data, use the API to retrieve the data. (https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\"The three pillars of a polite session are seeking permission, taking slowly and never asking twice\" (https://github.com/dmi3kno/polite)\n\n2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\nThe ROBOTS.TXT file defines what is allowed or not allowed within a website to web-crawling software (https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping). \nIt allows or prevents robots to access content based on the user's specifications. There is no standard, so it is truly up to the website creater to state what is allowed (or probably more specifically NOT allowed).\n\n\n3. **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\nUsing the example on the polite package's github: www.cheese.com\nI successfully installed polite and ran the below code (the basic example from the github page).\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_f82355d575ee2af345f54ab470065bce'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\n\nsession <- bow(\"https://www.cheese.com/by_type\", force = TRUE)\nresult <- scrape(session, query=list(t=\"semi-soft\", per_page=100)) %>%\n  html_node(\"#main-body\") %>% \n  html_nodes(\"h3\") %>% \n  html_text()\nhead(result)\n```\n:::\n\n\n\nInstructions:\n\nSubmit to your repo. Make sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}