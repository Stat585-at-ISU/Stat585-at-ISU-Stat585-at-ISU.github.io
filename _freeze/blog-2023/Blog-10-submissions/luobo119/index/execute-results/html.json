{
  "hash": "050c8a2b11c9cf805b7fb3c154eb8b15",
  "result": {
    "markdown": "---\nauthor: \"ycb\"\ntitle: \"Web scraping etiquette\"\ncategories: \"Errors and warnings in packages\"\ndate: \"2023-04-06\"\noutput: github_document\n---\n\n \n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data. \nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are: \n\n  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n  - R package [polite](https://github.com/dmi3kno/polite)\n\n  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\nAfter reading through some of the ethics essays\nwrite a blog post addressing the following questions: \n\n1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n\n* Do the scraping off-peak hours and request the data at a reasonable rate.\n\n* Let the website’s administrator know who you are, your intentions as well as your contact information by adding some strings in the user-agent.\n\n* Read ROBOTS.TXT and respect their rules.\n\nReferences: \n\n* https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping\n\n* https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01\n\n2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\n\nA ROBOTS.TXT file indicates what is allowed and what is not allowed in the web crawling. It is part of the Robot Exclusion Protocol (REP).\n\nIn the robots.txt for https://www.costco.com, part of the file is as follows:\n\nUser-agent: \\*\n\nDisallow: /\\*?\\*pageSize=\\*\n\nAllow: /\\*?currentPage=\\*pageSize=\\*\n\n which means costco.com does not want any use-agent to crawl the url /\\*?\\*pageSize=\\*, but it is allowed to crawl the url /\\*?currentPage=\\*pageSize=\\*.\n \n \n3. **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_15fa02c975ab262c2dceccedee0565e0'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\nlibrary(readr)\nlibrary(dplyr)\nsession <- bow(\"https://www.nytimes.com/elections/2016/results/iowa\", force = TRUE)\ntbl <- scrape(session) %>% html_table(fill=TRUE) \nia_results <- tbl[[2]] %>% mutate(\n  Trump  = parse_number(Trump),\n  Clinton = parse_number(Clinton)\n)\nhead(ia_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  `Vote by county` Trump Clinton\n  <chr>            <dbl>   <dbl>\n1 Polk             93492  119804\n2 Linn             48390   58935\n3 Scott            39149   40440\n4 Johnson          21044   50200\n5 Black Hawk       27476   32233\n6 Story            19458   25709\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}