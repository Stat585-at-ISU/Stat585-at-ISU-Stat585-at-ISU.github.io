{
  "hash": "6b808ea98021f0f131d199581f0a62cf",
  "result": {
    "markdown": "---\nauthor: \"Yingchao Zhou\"\ntitle: \"Web scraping etiquette ...\"\ncategories: \"Web scraping etiquette ...\"\ndate: \"2023-04-06\"\noutput: github_document\n---\n\n \n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data. \nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are: \n\n  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n  - R package [polite](https://github.com/dmi3kno/polite)\n\n  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\nAfter reading through some of the ethics essays\nwrite a blog post addressing the following questions: \n\n1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n\nFrom [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) and [polite](https://github.com/dmi3kno/polite):\n\n- Do not become a burden to the website one is scraping from. If there is a public API containing the desired data, then scraping does not need to happen. After `bow` to the host and get permission first, only `nod` is needed in the following process.\n\n- Be open about the scraper's identity by providing a user agent string and respond to the web owner's contact.\n\n- Respect the PI of the website. Only keep the data necessary to the project, and do not pass it as if the scraper owns it. Give credit to the website.\n\n2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\n\nFrom [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping): A ROBOTS.txt file indicates the web-crawling software where it is allowed (or not allowed) within the website. This is part of the Robots Exclusion Protocol (REP) which are a group of web standards created as a way to regulate how robots crawl the web.\n\nI checked the robots.txt file for overleaf. \n```\n# robots.txt for https://www.sharelatex.com/\n\nUser-agent: *\nDisallow: /project/*\nDisallow: /github/repos/*\nDisallow: /recurly.com\nDisallow: /user/password/set\nAllow: /\n\nUser-Agent: AhrefsBot\nDisallow: /\n\nUser-Agent: XoviBot\nDisallow: /\n\nUser-Agent: RankSonicBot\nDisallow: /\n\nUser-Agent: SMTBot\nDisallow: /\n```\n\nIt prevents all users (if not logged in I guess) from accessing the pages of  projects, github repos, recurly.com and setting password page. It also disallows AhrefsBot, XoviBot, RankSonicBot and SMTBot from visiting the overleaf website.\n\n3. **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\n\nI want to scrape the wikipedia for hummingbirds to retrieve the scientific classification of humming birds.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_f960a16e089ca95d95e946264b1aa50e'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\nlibrary(purrr)\n\nsession <- bow(\"https://en.wikipedia.org/wiki/Hummingbird\", force = TRUE)\nresult <- scrape(session)\n\ninfo <- result %>% \n  html_elements(xpath = '//table[@class=\"infobox biota\"]') %>%\n  html_table()\n\ninfo[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 2\n   HummingbirdTemporal range: Rupelian 30–0 Ma \\nPreꞒ\\nꞒ\\nO\\nS\\nD\\nC\\n…¹ Hummi…²\n   <chr>                                                                 <chr>  \n 1 \"\"                                                                    \"\"     \n 2 \"Four hummingbirdsfrom Trinidad and Tobago\"                           \"Four …\n 3 \"Scientific classification\"                                           \"Scien…\n 4 \"Kingdom:\"                                                            \"Anima…\n 5 \"Phylum:\"                                                             \"Chord…\n 6 \"Class:\"                                                              \"Aves\" \n 7 \"Clade:\"                                                              \"Stris…\n 8 \"Order:\"                                                              \"Apodi…\n 9 \"Family:\"                                                             \"Troch…\n10 \"Type genus\"                                                          \"Type …\n11 \"TrochilusLinnaeus, 1758\"                                             \"Troch…\n12 \"Subfamilies\"                                                         \"Subfa…\n13 \"†EurotrochilusFlorisuginaePhaethornithinaePolytminaeLesbiinaePatago… \"†Euro…\n# … with abbreviated variable names\n#   ¹​`HummingbirdTemporal range: Rupelian 30–0 Ma \\nPreꞒ\\nꞒ\\nO\\nS\\nD\\nC\\nP\\nT\\nJ\\nK\\nPg\\nN`,\n#   ²​`HummingbirdTemporal range: Rupelian 30–0 Ma \\nPreꞒ\\nꞒ\\nO\\nS\\nD\\nC\\nP\\nT\\nJ\\nK\\nPg\\nN`\n```\n:::\n:::\n\n\nInstructions:\n\nSubmit to your repo. Make sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}