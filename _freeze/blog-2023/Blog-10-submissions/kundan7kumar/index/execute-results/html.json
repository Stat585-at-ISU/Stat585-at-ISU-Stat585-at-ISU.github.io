{
  "hash": "82c79e2bf0b24ba80788b7d43b758486",
  "result": {
    "markdown": "---\nauthor: \"Kundan Kumar\"\ntitle: \"Web scraping\"\ncategories: \"Errors and warnings in packages\"\ndate: \"2023-04-06\"\noutput: github_document\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is\nbased on data and services that scrape those data. Now that we start to\napply scraping mechanisms, we need to think about how to apply those\nskills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you\nget started with that are:\n\n-   [James\n    Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n-   R package [polite](https://github.com/dmi3kno/polite)\n\n-   [JAMI \\@\n    EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\nAfter reading through some of the ethics essays write a blog post\naddressing the following questions:\n\n1.  **What are your main three takeaways for ethical web scraping? -\n    Give examples, or cite your sources.**\n\n**Solution:** The major takeaway for ethical web scraping :\n\n-   Before any web-Scarping, it is important to check the website's\n    terms of use and robots.txt file to ensure you are not violating any\n    rules.\n\n-   Web-Scarping should not collect sensitive or private information\n    without the consent of the website owner or the individuals\n    concerned. It should follow data privacy and intellectual property\n    like copyrighted material rights.\n\n-   Web-scraping should not cause undue load on the website's server or\n    disrupt its performance. It limits the rate at which requests are\n    made to the website's server.\n\n**References:**\n\n-   website's terms of use and robots.txt:\n    <https://developers.google.com/search/docs/crawling-indexing/robots/intro>\n    and\n    <https://techcrunch.com/2022/04/18/web-scraping-legal-court/?guccounter=1>\n-   Data privacy laws and web scraping:\n    <https://www.fieldfisher.com/en/services/privacy-security-and-information/privacy-security-and-information-law-blog/data-scraping-considering-the-privacy-issues>\n-   Limit request rate:\n    <https://blog.cloudflare.com/advanced-rate-limiting/>\n\n2.  **What is a ROBOTS.TXT file? Identify one instance and explain what\n    it allows/prevents.**\n\n**Solution:** A robots.txt file is placed on a website's server,\ninstructing web crawlers how to crawl and index its pages. It is a plain\ntext file specifying which parts of the website the web crawlers are\nallowed or not to access.\n\nAn example of a robots.txt file looks like this:\n\n    User-agent: *\n    Disallow: /wp-admin/\n    Allow: /wp-admin/admin-ajax.php\n\n**References:** Check for the complete robots.txt:\n<https://kinsta.com/robots.txt>\n\nThe `User-agent: *` line indicates what rules apply to all web-crawler.\nThe Disallow lines specify directories or pages not to be accessed by\nthe bots, while Allow lines allow the bot to access pages or directories\nby it.\n\nThis `Disallow` help to prevent the bot from crawling a page with\nsensitive information.\n\n3.  **Identify a website that you would like to scrape (or one an\n    example from class) and implement a scrape using the `polite`\n    package.**\n\n**Solution:**\nWe are doing web-scrape for Active Civil Service List data, which consists of all candidate which passed the exam.\n\nhttps://data.cityofnewyork.us/City-Government/Civil-Service-List-Active-/vx8i-nprf\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_39fc5fd46d0efe6db97bf02b070a3497'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\nlibrary(httr)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\npolite_GET <- politely(httr::GET,verbose = TRUE)\nres <- polite_GET(\"https://data.cityofnewyork.us/resource/vx8i-nprf.json\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nFetching robots.txt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nrt_robotstxt_http_getter: normal http get\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nNew copy robots.txt was fetched from https://data.cityofnewyork.us/robots.txt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nTotal of 1 crawl delay rule(s) defined for this host.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nYour rate will be set to 1 request every 5 second(s).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nPausing... \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nScraping: https://data.cityofnewyork.us/resource/vx8i-nprf.json\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting useragent: polite R (4.2.1 x86_64-pc-linux-gnu x86_64 linux-gnu) bot\n```\n:::\n\n```{.r .cell-code}\n#res\ndf <- jsonlite::fromJSON(rawToChar(res$content))\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  exam_no list_no first_name last_name adj_fa list_title_code\n1    0162 875.000    JENELLE    FRASER  78.00           10001\n2    0162 876.000        AML  METRYOSE  78.00           10001\n3    0162 877.000    YAHAIRA   ALMONTE  78.00           10001\n4    0162 878.000    CHI SUN      CHOW  77.00           10001\n5    0162 879.000     RACHEL    CANCEL  77.00           10001\n6    0162 880.000    FARHANA     AKTER  76.00           10001\n            list_title_desc group_no list_agency_code list_agency_desc\n1 ADMINISTRATIVE ACCOUNTANT      000              000 OPEN COMPETITIVE\n2 ADMINISTRATIVE ACCOUNTANT      000              000 OPEN COMPETITIVE\n3 ADMINISTRATIVE ACCOUNTANT      000              000 OPEN COMPETITIVE\n4 ADMINISTRATIVE ACCOUNTANT      000              000 OPEN COMPETITIVE\n5 ADMINISTRATIVE ACCOUNTANT      000              000 OPEN COMPETITIVE\n6 ADMINISTRATIVE ACCOUNTANT      000              000 OPEN COMPETITIVE\n           published_date        established_date        anniversary_date   mi\n1 2021-05-26T00:00:00.000 2021-07-28T00:00:00.000 2025-07-28T00:00:00.000 <NA>\n2 2021-05-26T00:00:00.000 2021-07-28T00:00:00.000 2025-07-28T00:00:00.000    T\n3 2021-05-26T00:00:00.000 2021-07-28T00:00:00.000 2025-07-28T00:00:00.000 <NA>\n4 2021-05-26T00:00:00.000 2021-07-28T00:00:00.000 2025-07-28T00:00:00.000 <NA>\n5 2021-05-26T00:00:00.000 2021-07-28T00:00:00.000 2025-07-28T00:00:00.000 <NA>\n6 2021-05-26T00:00:00.000 2021-07-28T00:00:00.000 2025-07-28T00:00:00.000 <NA>\n  veteran_credit sibling_lgy_credit\n1           <NA>               <NA>\n2           <NA>               <NA>\n3           <NA>               <NA>\n4           <NA>               <NA>\n5           <NA>               <NA>\n6           <NA>               <NA>\n```\n:::\n:::\n\n\nTotal Candidate passed for different agency\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_031fe0eadee7bd4048f3430e0ac94d5b'}\n\n```{.r .cell-code}\ndf_grouped <- df %>%\n  group_by(list_agency_desc) %>%\n  summarize(total_person = n())\n\ndf_grouped\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 9 Ã— 2\n  list_agency_desc                       total_person\n  <chr>                                         <int>\n1 ADMINISTRATION FOR CHILDREN'S SERVICES            2\n2 DEPARTMENT OF CITY PLANNING                       1\n3 DEPARTMENT OF EDUCATION                          25\n4 HRA/DEPARTMENT OF SOCIAL SERVICES                 4\n5 NYC EMPLOYEES' RETIREMENT SYSTEM                  5\n6 OFFICE OF THE COMPTROLLER                        19\n7 OPEN COMPETITIVE                                932\n8 POLICE DEPARTMENT                                 9\n9 TEACHERS' RETIREMENT SYSTEM                       3\n```\n:::\n:::\n\n\n\nInstructions:\n\nSubmit to your repo. Make sure that all of the github actions pass\n(check menu item Actions - all of the actions should have green checks)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}