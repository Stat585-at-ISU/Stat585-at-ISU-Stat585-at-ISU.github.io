{
  "hash": "3a76cef26c6cfb7975e33fbaafae8ec2",
  "result": {
    "markdown": "---\nauthor: \"CdS\"\ntitle: \"Web scraping etiquette ...\"\ncategories: \"Web scraping etiquette ...\"\ndate: \"2023-04-06\"\noutput: github_document\neditor_options: \n  markdown: \n    wrap: sentence\n---\n\n\n\n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data.\nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are:\n\n-   [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n-   R package [polite](https://github.com/dmi3kno/polite)\n\n-   [JAMI \\@ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\nAfter reading through some of the ethics essays write a blog post addressing the following questions:\n\n1.  **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n2.  **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\n3.  **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\n\nInstructions:\n\nSubmit to your repo.\nMake sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)\n\n## Answers:\n\n1 - **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n\nThere are three main takeaways related to ethical web scraping that caught my attention.\nThe first one is to be respectful to the owner of the data one is scraping.\nFor example, the web scraping operation might be very demanding on the server so trying to execute this operation as smoothly as possible would be my first aim.\nThis can be done by minimizing the amount of requests sent to the server.\nThe second aspect that caught my attention was to be also respectful to the human users of the website one is scraping.\nFor example, the idea of web scraping only outside of peak use hours is fascinating to me.\nIt reduces the traffic on the server and also does not interfere with the user experience.\nThe third and last aspect that I thought was extremely interesting was the idea that, if an Application Programming Interface (API) is provided, to avoid web scraping all together.\nThis API will most likely be more equiped to handle large requests than the website front end.\n\nAn example of how to apply a couple of these principles would be on how to request weather data from [NASA POWER](https://power.larc.nasa.gov/). Often I will have to send multiple requests for weather data but it is important to try and minimize the amount of requests.\nLet us take a look:\n\nIf we need to iterate over a list of locations for which we need data for multiple years, for example, there are multiple ways to do it, but some of them will be more gentle to the API server.\nIn the following example, we can see how a simple tweak in the code can change the number of requests to the server from 62 to 2.\n\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_6288fb90d46bf415d485b554e4bfb5d1'}\n\n```{.r .cell-code}\n# renv::init()\n# renv::activate()\n# renv::restore()\nlocations <- data.frame(location_name = c('Ames, IA', 'Fayetteville, AR'),\n                        latitude = c(42, 36),\n                        longitude = c(94, 94))\n\nlocations <- merge(locations, \n                   data.frame(year = 1990:2020))\n\nhead(locations)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     location_name latitude longitude year\n1         Ames, IA       42        94 1990\n2 Fayetteville, AR       36        94 1990\n3         Ames, IA       42        94 1991\n4 Fayetteville, AR       36        94 1991\n5         Ames, IA       42        94 1992\n6 Fayetteville, AR       36        94 1992\n```\n:::\n\n```{.r .cell-code}\ndim(locations)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 62  4\n```\n:::\n:::\n\n\nIf we need the data for each of these locations at each one of these years, one might be inclined to do the following loop:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_de2b464e434116206e53f70b21e67b13'}\n\n```{.r .cell-code}\nweather_all <- data.frame()\ntime.start <- Sys.time()\nfor (i in 1:dim(locations)[1]) {\n  \n  dates <- c(sprintf('%1$s-01-01', locations$year[i]),\n             sprintf('%1$s-12-31', locations$year[i]))\n  \n  lonlat <- c(locations$longitude[i], locations$latitude[i])\n  \n  weather_ind <- nasapower::get_power(community = \"AG\", pars = c(\"T2M_MAX\", \n    \"T2M_MIN\", \"ALLSKY_SFC_SW_DWN\", \"PRECTOTCORR\", \"RH2M\", \n    \"WS2M\"), dates = dates, lonlat = lonlat, temporal_api = \"daily\")\n  \n  weather_ind$location_name <- locations$location_name[i]\n  weather_all <- rbind(weather_all, weather_ind)\n \n}\ntime.end <- Sys.time()\nprint(time.end - time.start)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 3.012721 mins\n```\n:::\n:::\n\n\nThe previous code would have sent 62 separate requests to the NASA POWER server. The following loop will retrieve the same information but with only 3% of the requests. This example uses an API but the same principle can be applied to the front face of a website. \n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_cd79c9895e0350d7a0e64f599c5abd46'}\n\n```{.r .cell-code}\nsearch_parameters <- do.call(rbind, \n                            by(locations, locations$location_name, \n                               function(df) data.frame(df[1, c('location_name', 'longitude', 'latitude')],\n                               min_year = min(df$year), \n                               max_year = max(df$year))))\n\ntime.start <- Sys.time()\n\n\nweather_all <- data.frame()\n\nfor (i in 1:dim(search_parameters)[1]) {\n  \n  dates <- c(sprintf('%1$s-01-01', search_parameters$min_year[i]),\n             sprintf('%1$s-12-31', search_parameters$max_year[i]))\n  \n  lonlat <- c(search_parameters$longitude[i], search_parameters$latitude[i])\n  \n  weather_ind <- nasapower::get_power(community = \"AG\", pars = c(\"T2M_MAX\", \n    \"T2M_MIN\", \"ALLSKY_SFC_SW_DWN\", \"PRECTOTCORR\", \"RH2M\", \n    \"WS2M\"), dates = dates, lonlat = lonlat, temporal_api = \"daily\")\n  \n  weather_ind$location_name <- search_parameters$location_name[i]\n  weather_all <- rbind(weather_all, weather_ind)\n \n}\ntime.end <- Sys.time()\nprint(time.end - time.start)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 7.605049 secs\n```\n:::\n:::\n\n\n2 -  **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.** \n\nThe robots.txt file is a file that tells bots instructions of where the bot is allowed to crawl or not. It is a convention and it is a sign of good faith to respect the robots.txt file. The file allows or disallows certain behaviors of bots. These rules can be implemented globally or for specific bots.\n\nAn example is the robots.txt file from [https://www.iastate.edu](https://www.iastate.edu/robots.txt).\nIt specific disallows not clean URLs.\n\n    # Paths (no clean URLs)\n    Disallow: /index.php/admin/\n    Disallow: /index.php/comment/reply/\n    Disallow: /index.php/filter/tips\n    Disallow: /index.php/node/add/\n    Disallow: /index.php/search/\n    Disallow: /index.php/user/password\n    Disallow: /index.php/user/register\n    Disallow: /index.php/user/login\n    Disallow: /index.php/user/logout\n    Disallow: /index.php/media/oembed\n    Disallow: /index.php/*/media/oembed\n\n3-  **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_bda5a81ff5d9e859549cfb3aa5b13a87'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\nlibrary(readr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'readr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:rvest':\n\n    guess_encoding\n```\n:::\n\n```{.r .cell-code}\nsession <- bow(\"https://www.nytimes.com/elections/2016/results/iowa\", force = TRUE)\nresult <- scrape(session, query=list(t=\"semi-soft\", per_page=100))\n\ntables <- result %>% html_table(fill=TRUE)\ntables[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 99 × 3\n   `Vote by county` Trump  Clinton\n   <chr>            <chr>  <chr>  \n 1 Polk             93,492 119,804\n 2 Linn             48,390 58,935 \n 3 Scott            39,149 40,440 \n 4 Johnson          21,044 50,200 \n 5 Black Hawk       27,476 32,233 \n 6 Story            19,458 25,709 \n 7 Dubuque          23,460 22,850 \n 8 Woodbury         24,727 16,210 \n 9 Pottawattamie    24,447 15,355 \n10 Dallas           19,339 15,701 \n# … with 89 more rows\n```\n:::\n\n```{.r .cell-code}\nia_results <- tables[[2]] \nia_results$Trump <- with(ia_results,  parse_number(Trump))\nia_results$Clinton <- with(ia_results,  parse_number(Clinton))\n\nhead(ia_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  `Vote by county` Trump Clinton\n  <chr>            <dbl>   <dbl>\n1 Polk             93492  119804\n2 Linn             48390   58935\n3 Scott            39149   40440\n4 Johnson          21044   50200\n5 Black Hawk       27476   32233\n6 Story            19458   25709\n```\n:::\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-5_38fe079502d80bdfb9ff56a01d42be9e'}\n\n```{.r .cell-code}\n# renv::snapshot()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}