{
  "hash": "599e9ffa6931aea6213cc45a38195779",
  "result": {
    "markdown": "---\nauthor: \"Ian Parzyszek\"\ntitle: \"Web scraping etiquette ...\"\ncategories: \"Web scraping etiquette ...\"\ndate: \"2023-04-06\"\noutput: github_document\n---\n\n \n## Prompt:\n\nWith great power comes great responsibility - a large part of the web is based on data and services that scrape those data. \nNow that we start to apply scraping mechanisms, we need to think about how to apply those skills without becoming a burden to the internet society.\n\nFind sources on ethical web scraping - some readings that might help you get started with that are: \n\n  - [James Densmore](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n  - R package [polite](https://github.com/dmi3kno/polite)\n\n  - [JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\nAfter reading through some of the ethics essays\nwrite a blog post addressing the following questions: \n\n1. **What are your main three takeaways for ethical web scraping? - Give examples, or cite your sources.**\n\nOne of the first things I was unaware of was that webscraping can be a burden on the server. I did not think it was anymore cumbersome for the server than just visiting the website, but because this is not the case good practice is to conduct scraping during non-busy times.\n\nAnother good practice is to identify yourself. The websites owner may see some unusual activity, so it can be a good idea to give a string in your code to identify yourself, and maybe also let them know your intentions.\n\nLastly, you should give back to the website owner and give them credit. If you are using their data, cite their website/article. This will help give their website some more traffic.\n\nI got these takeaways from :[JAMI @ EMPIRICAL](https://www.empiricaldata.org/dataladyblog/a-guide-to-ethical-web-scraping)\n\n\n2. **What is a ROBOTS.TXT file? Identify one instance and explain what it allows/prevents.**\n\nA ROBOTS.TXT files is put in place to put limitations on what crawlers can access on their website. An example would be including a ROBOTS.TXT file to limit the amount of information a search engine can search on your website and include in their results. Sometimes you may want GOOGLE to avoid including PDFs or pictures, so you could include a ROBOTS.TXT file to prevent this.\n\n3. **Identify a website that you would like to scrape (or one an example from class) and implement a scrape using the `polite` package.**\n\nInstructions:\n\nSubmit to your repo. Make sure that all of the github actions pass (check menu item Actions - all of the actions should have green checks)\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-1_339eb83d46d4282ebc8d842f201abb98'}\n\n```{.r .cell-code}\nlibrary(polite)\nlibrary(rvest)\nsession <-  bow(\"https://www.avca.org/polls/diii-men/2023/03-28-23.html\")\nscrape(session) %>%\n  html_table()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo encoding supplied: defaulting to UTF-8.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n# A tibble: 15 × 5\n   Rank  School (First-Place Vot…¹ Total Points Adjuste…² Record `Previous Rank`\n   <chr> <chr>                                      <int> <chr>            <int>\n 1 1     Stevens [20]                                 342 26-2                 1\n 2 2     Vassar [3]                                   325 17-1                 2\n 3 3     Juniata                                      294 22-2                 3\n 4 4     Springfield                                  276 20-2                 4\n 5 5     Messiah                                      251 20-2                 5\n 6 6     North Central (IL)                           217 18-3                 7\n 7 T-7   NYU                                          171 12-6                 9\n 8 T-7   SUNY New Paltz                               171 18-8                11\n 9 9     Carthage                                     168 14-5                 6\n10 10    Southern Virginia                            167 13-2                10\n11 11    St John Fisher                               135 19-6                 8\n12 12    Wentworth                                     64 18-6                13\n13 13    Nazareth                                      58 20-5                14\n14 14    Marymount                                     56 17-5                12\n15 15    Rutgers-Newark                                31 11-5                15\n# ℹ abbreviated names: ¹​`School (First-Place Votes Adjusted)`,\n#   ²​`Total Points Adjusted`\n```\n:::\n\n```{.r .cell-code}\n#Here is the Division 3 Men's Volleyball National Rankings...Messiah University is ranked 5! Go Falcons!!!!!\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}