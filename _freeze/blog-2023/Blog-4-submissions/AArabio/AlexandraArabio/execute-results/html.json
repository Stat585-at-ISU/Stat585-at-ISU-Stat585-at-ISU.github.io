{
  "hash": "44a1b42876c83b14b09f60eb90ca6008",
  "result": {
    "markdown": "---\nauthor: \"Alexandra Arabio\"\ntitle: \"Split, Apply, Combine - Relevancy\"\ncategories: Split-apply-combine\ndate: 2023-02-16\noutput: github_document\n---\n\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n\nCitation used for this blog: \n\nWickham, H. . (2011). The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software, 40(1), 1–29. https://doi.org/10.18637/jss.v040.i01\n\n\n## Prompt:\n\nThe `plyr` package has by now been replaced by other, even faster\npackages, but the idea of *Split, apply, combine* is as relevant as\never.\n\nRead the paper [The Split-Apply-Combine Strategy for Data\nAnalysis](https://www.jstatsoft.org/article/view/v040i01) by Hadley\nWickham.\n\nWrite a blog post addressing the following questions:\n\n1.  The R code for the split-apply-combine paper is posted with the\n    paper. Pick one of the examples demonstrating `plyr` functionality\n    (such as `dlply` or `ddply`, ...) and rewrite the example using\n    functionality from the package `dplyr`. Make sure that your example\n    works and the results are identical.\n\n\n\n::: {.cell hash='AlexandraArabio_cache/html/unnamed-chunk-2_7f54699000099ee2cc71ac72cdb84f05'}\n\n```{.r .cell-code}\n#plyr functionality using dplyr\nlibrary(\"plyr\")\nlibrary(\"dplyr\")\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:plyr':\n#> \n#>     arrange, count, desc, failwith, id, mutate, rename, summarise,\n#>     summarize\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(\"tidyverse\")\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2\n#> ──\n#> ✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n#> ✔ tibble  3.1.8     ✔ stringr 1.5.0\n#> ✔ tidyr   1.3.0     ✔ forcats 0.5.2\n#> ✔ readr   2.1.3     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::arrange()   masks plyr::arrange()\n#> ✖ purrr::compact()   masks plyr::compact()\n#> ✖ dplyr::count()     masks plyr::count()\n#> ✖ dplyr::desc()      masks plyr::desc()\n#> ✖ dplyr::failwith()  masks plyr::failwith()\n#> ✖ dplyr::filter()    masks stats::filter()\n#> ✖ dplyr::id()        masks plyr::id()\n#> ✖ dplyr::lag()       masks stats::lag()\n#> ✖ dplyr::mutate()    masks plyr::mutate()\n#> ✖ dplyr::rename()    masks plyr::rename()\n#> ✖ dplyr::summarise() masks plyr::summarise()\n#> ✖ dplyr::summarize() masks plyr::summarize()\n# ----------------------------\n\n# Base ball case study ============================================\nbaberuth <- subset(baseball, id == \"ruthba01\")\nbaberuth <- transform(baberuth, cyear = year - min(year) + 1)\n\n\n# ----------------------------\n\nbaseball <- ddply(baseball, .(id), transform, \n  cyear = year - min(year) + 1)\n\n\n# ----------------------------\n\n##ddply splits a data frame, applies a function, and then returns results in a data frame ...\n\nbaberuth <- subset(baseball, id == \"ruthba01\")\nbaberuth <- transform(baberuth, cyear = year - min(year) + 1)\n\nbaberuth1 <- subset(baseball, id == \"ruthba01\")\nbaberuth1 <- transform(baberuth, cyear = year - min(year) + 1)\n\n# ----------------------------\n\nbaseball <- ddply(baseball, .(id), transform, \n  cyear = year - min(year) + 1)\n\n#dplyr with do()\n\nbaseball1 <- baberuth1 %>% \n  group_by(id) %>% \n  do(cyear = baberuth1$year - min(baberuth1$year) + 1)\n```\n:::\n\n\n\n2.  Which (base R) functions do you know that support the\n    split-apply-combine strategy? In your opinion, are these sufficient?\n    State why or why not?\n\nThere are many different functions within base R that support the split-apply-combine strategy. Hadley touches on this discussion in the beginning parts of her paper; one of the most commonly used functions for this in base R is probably the aggregate function. This function works by combining or grouping data by common values of a factor to then generate summary statistics or apply functions for those groups. \n\nAs Hadley mentions through, the readability of some base R functions are much more messy as compared to ones that could be used through other packages. In a package like dplyr, we are able to utilize the group_by and summarise functions to complete the same task but produce more readable output and workflow. Whereas the aggregate functions only allows one to apply a single names function to a grouped data the summarise function allows for more functions to be used. \n\nBy using this step by step method through grouping and running summaries... the dplyr functions are able to be used together to accomplish more tasks. \n\n In my opinion, I think that there are many different base R functions that support the split_apply-combine strategy that Hadley discusses... however, functions in the dplyr package accomplish the same types of tasks and are generally more easily understand.\n\n3. The ChatGPT response to the prompt \"Describe the split-apply-combine paradigm\" is given below. Generally, the chatbot's answers are decent. ChatGPT does have problems getting details right. Which details would you change in the response? \n\n    \n>    The \"split-apply-combine\" paradigm is a data analysis strategy used in statistics and data science to analyze grouped data. It consists of three steps:\n>\n>    Split: The data is first split into groups based on one or more variables of interest.\n>\n>    Apply: A specific operation or function is applied to each group of data, usually for the purpose of aggregating, summarizing, or transforming the data within each group.\n>\n> Combine: The results of the operation applied to each group are then combined and returned as a single output.\n>\n> The split-apply-combine paradigm is particularly useful in cases where the data cannot be analyzed as a whole and instead must be analyzed in smaller, more manageable chunks. This allows for more efficient data processing and improved scalability. The paradigm is commonly implemented using tools such as the \"groupby\" function in the R and Python programming languages, and the \"dplyr\" library in R.\n\nI do think that the ChatGPT response to the prompt is pretty good. The split-apply-combine does allow one to break down a larger problem into more manageable parts to work with each individually before then combining the dataset together again. Working through this format limits the chance of errors, is less time consuming, and is not as tedious as other methods may be. \n\nYou can write your answers directly the `README.Rmd` file. \nMake sure that the file knits (you will need to activate any packages your use in your code chunks with a call to `library(xxx)`, where xxx is the name of the package, such as `plyr` ).\nCommit your changes and push to your repo;  add any files in the `README_files` directory to your repository. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}