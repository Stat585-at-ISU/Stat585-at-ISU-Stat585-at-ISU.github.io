{
  "hash": "e35fa920a09b928c946f11b3952f5f7f",
  "result": {
    "markdown": "---\nauthor: \"Harun Celik\"\ntitle: \"Split-Apply-Combine Post\"\ncategories: Split-apply-combine\ndate: 2023-02-16\noutput: github_document\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n\n\n## Prompt:\n\nThe `plyr` package has by now been replaced by other, even faster\npackages, but the idea of *Split, apply, combine* is as relevant as\never.\n\nRead the paper [The Split-Apply-Combine Strategy for Data\nAnalysis](https://www.jstatsoft.org/article/view/v040i01) by Hadley\nWickham.\n\nWrite a blog post addressing the following questions:\n\n1.  The R code for the split-apply-combine paper is posted with the\n    paper. Pick one of the examples demonstrating `plyr` functionality\n    (such as `dlply` or `ddply`, ...) and rewrite the example using\n    functionality from the package `dplyr`. Make sure that your example\n    works and the results are identical.\n\n2.  Which (base R) functions do you know that support the\n    split-apply-combine strategy? In your opinion, are these sufficient?\n    State why or why not?\n\n3.  The ChatGPT response to the prompt \"Describe the split-apply-combine\n    paradigm\" is given below. Generally, the chatbot's answers are\n    decent. ChatGPT does have problems getting details right. Which\n    details would you change in the response?\n\n> The \"split-apply-combine\" paradigm is a data analysis strategy used in\n> statistics and data science to analyze grouped data. It consists of\n> three steps:\n>\n> Split: The data is first split into groups based on one or more\n> variables of interest.\n>\n> Apply: A specific operation or function is applied to each group of\n> data, usually for the purpose of aggregating, summarizing, or\n> transforming the data within each group.\n>\n> Combine: The results of the operation applied to each group are then\n> combined and returned as a single output.\n>\n> The split-apply-combine paradigm is particularly useful in cases where\n> the data cannot be analyzed as a whole and instead must be analyzed in\n> smaller, more manageable chunks. This allows for more efficient data\n> processing and improved scalability. The paradigm is commonly\n> implemented using tools such as the \"groupby\" function in the R and\n> Python programming languages, and the \"dplyr\" library in R.\n\nYou can write your answers directly the `README.Rmd` file. Make sure\nthat the file knits (you will need to activate any packages your use in\nyour code chunks with a call to `library(xxx)`, where xxx is the name of\nthe package, such as `plyr` ). Commit your changes and push to your\nrepo; add any files in the `README_files` directory to your repository.\n\n#Answers\n\n## Question One\n\n### Code Selection\n\nI wanted to work with a simple example from Hadley Wickham's code so\nthat I could better understand in detail what was happening. For this\nreason, I chose to work with a chunk of code that used `ddply` to first\ngroup_by the column `$id`, and then to create a new column called\n`$cyear` using a base R `transform()` function.\n\n### Here is the original code in `plyr`:\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_87d67a6d63b9ee8c3805176751922974'}\n\n```{.r .cell-code}\nlibrary(\"ggplot2\")\nlibrary(\"plyr\")\noptions(digits = 3)\noptions(prompt = \"R> \")\n```\n:::\n\n::: {.cell hash='index_cache/html/unnamed-chunk-3_cb0280eb880610b4dde577eb67a137d8'}\n\n```{.r .cell-code}\n#Changed variable name to baseball_plyr\nbaseball_plyr <- ddply(baseball, .(id), transform, \n  cyear = year - min(year) + 1)\nhead(baseball_plyr)\n#>          id year stint team lg   g  ab   r   h X2b X3b hr rbi sb cs bb so ibb\n#> 1 aaronha01 1954     1  ML1 NL 122 468  58 131  27   6 13  69  2  2 28 39  NA\n#> 2 aaronha01 1955     1  ML1 NL 153 602 105 189  37   9 27 106  3  1 49 61   5\n#> 3 aaronha01 1956     1  ML1 NL 153 609 106 200  34  14 26  92  2  4 37 54   6\n#> 4 aaronha01 1957     1  ML1 NL 151 615 118 198  27   6 44 132  1  1 57 58  15\n#> 5 aaronha01 1958     1  ML1 NL 153 601 109 196  34   4 30  95  4  1 59 49  16\n#> 6 aaronha01 1959     1  ML1 NL 154 629 116 223  46   7 39 123  8  0 51 54  17\n#>   hbp sh sf gidp cyear\n#> 1   3  6  4   13     1\n#> 2   3  7  4   20     2\n#> 3   2  5  7   21     3\n#> 4   0  0  3   13     4\n#> 5   1  0  3   21     5\n#> 6   4  0  9   19     6\n```\n:::\n\n\nHere is the output replicated in `dplyr`:\n\n\n::: {.cell hash='index_cache/html/load dplyr_89b810a0f5a0a32b6ba5edca5a93d48b'}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n:::\n\n::: {.cell hash='index_cache/html/replicate in dplyr_60b364a9b676741f7e1978f7244ce302'}\n\n```{.r .cell-code}\nbaseball_dplyr <- baseball %>%\n  group_by(id) %>%\n  mutate(cyear = year - min(year) + 1)\nhead(baseball_dplyr)\n#> # A tibble: 6 × 23\n#> # Groups:   id [6]\n#>   id      year stint team  lg        g    ab     r     h   X2b   X3b    hr   rbi\n#>   <chr>  <int> <int> <chr> <chr> <int> <int> <int> <int> <int> <int> <int> <int>\n#> 1 anson…  1871     1 RC1   \"\"       25   120    29    39    11     3     0    16\n#> 2 force…  1871     1 WS3   \"\"       32   162    45    45     9     4     0    29\n#> 3 mathe…  1871     1 FW1   \"\"       19    89    15    24     3     1     0    10\n#> 4 start…  1871     1 NY2   \"\"       33   161    35    58     5     1     1    34\n#> 5 sutto…  1871     1 CL1   \"\"       29   128    35    45     3     7     3    23\n#> 6 white…  1871     1 CL1   \"\"       29   146    40    47     6     5     1    21\n#> # … with 10 more variables: sb <int>, cs <int>, bb <int>, so <int>, ibb <int>,\n#> #   hbp <int>, sh <int>, sf <int>, gidp <int>, cyear <dbl>\n```\n:::\n\n\n### Discussion\n\nIn both cases, the input and output is a data frame. While the code is\nless intuitive to understand at first glance in `ddply` the input/output\nformats are represented easier through the `dd` notation. Both codes\ngroup by `$id` and create a new column that is called `$cyear` with the\ncumulative year of a player's career. The `dplyr` package makes use of\nthe pipeline operator `%>%` to streamline the process of\nsplit-apply-combine whereas `ddply` looks more similar to how the\n`apply` functions take care of this paradigm in base r. One interesting\ndifference that I never paid attention to before was that the `str()`\nfunction produces different results because of the `group_by()` function\nin `dplyr`. It looks like the `group_by()` function creates nested\nstructures when calling `str()` on the data frame.\n\n## Question Two\n\n### Discussion\n\nUntil recently, I wasn't following the differences between `base r` and\nthe `tidyverse` so the only base function I am aware of that follows\nthis split-apply-combine paradigm are the `apply` functions.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-4_b1e6a63f0c067d890b207528bc3c3e45'}\n\n```{.r .cell-code}\nby(baseball$g, baseball$id, mean) |>\n  head()\n#> baseball$id\n#> aaronha01 abernte02 adairje01 adamsba01 adamsbo03 adcocjo01 \n#>     143.4      40.1      77.7      25.4      85.4     115.2\n```\n:::\n\n\nThe code above uses `by` which serves as a wrapper for `tapply` and\nwhile it does calculate the mean of `$g` by `$id`, the information of\nthe output is not as detailed as when run with the `dplyr` functions. I\nalso found it difficult to work with the syntax of the new base pipe\n`|>` operator despite it serving a similar functionality to the more\nwidely utilized `magrittr` pipe operator.\n\n## Question Three\n\nThroughout the article, Hadley Wickham demonstrates the importance of\nworking with the split-apply-combine paradigm efficiently. This is\nsomething that ChatGPT did not pick up on in their answer. The answer\nfrom the AI for instance doesn't mention that in the \"split\" section,\nmemory-saving methods, like the ones introduced by `plyr`, are an\nimportant process of working through the paradigm. The AI also doesn't\ndiscuss the limitations of the split-apply-combine method. An important\nlimitation to the paradigm, especially through use of `plyr` is that\n\"each piece of data [is] processed only once and independently of all\nother pieces\" (Wickham, 2011). This is an important element of the\nparadigm that separates it from advanced looping functions.\n\nI have to admit that ChatGPT's answer is surprisingly concise even if it\nis a little generalized. It would be interesting to see how it answers\nmore particular questions about the paradigm like, \"what are the\nlimitations of the split-apply-combine paradigm?\"\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}