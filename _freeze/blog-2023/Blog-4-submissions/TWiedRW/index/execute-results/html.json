{
  "hash": "e48a179ae5b84adb0bee7bc9c961ccbe",
  "result": {
    "markdown": "---\nauthor: \"Tyler Wiederich\"\ntitle: \"Split-apply-combine\"\ncategories: Split-apply-combine\ndate: 2023-02-16\noutput: github_document\neditor_options: \n  markdown: \n    wrap: 72\n---\n\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n\n\n## Prompt:\n\nThe `plyr` package has by now been replaced by other, even faster\npackages, but the idea of *Split, apply, combine* is as relevant as\never.\n\nRead the paper [The Split-Apply-Combine Strategy for Data\nAnalysis](https://www.jstatsoft.org/article/view/v040i01) by Hadley\nWickham.\n\nWrite a blog post addressing the following questions:\n\n**1.  The R code for the split-apply-combine paper is posted with the\n    paper. Pick one of the examples demonstrating `plyr` functionality\n    (such as `dlply` or `ddply`, ...) and rewrite the example using\n    functionality from the package `dplyr`. Make sure that your example\n    works and the results are identical.**\n    \n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_3103fd6d252df7c82c04093d18994fa4'}\n\n```{.r .cell-code}\n\n#plyr approach\nlibrary(plyr)\n\n#Year with plyr\nbaberuth <- subset(baseball, id == \"ruthba01\")\nbaberuth <- transform(baberuth, cyear = year - min(year) + 1)\nbaseball <- ddply(baseball, .(id), transform,\n  cyear = year - min(year) + 1)\nbaseball_subset <- subset(baseball, ab >= 25)\n\n#Models with plyr\nmodel <- function(df) {\n lm(rbi / ab ~ cyear, data = df)\n}\nbmodels <- dlply(baseball_subset, .(id), model)\nrsq <- function(x) summary(x)$r.squared\nbcoefs <- ldply(bmodels, function(x) c(coef(x), rsquare = rsq(x)))\nnames(bcoefs)[2:3] <- c(\"intercept\", \"slope\")\nbaseballcoef <- merge(baseball_subset, bcoefs, by = \"id\")\nresults = subset(baseballcoef, rsquare > 0.999)$id\n\n\n\n#dplyr approach\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:plyr':\n#> \n#>     arrange, count, desc, failwith, id, mutate, rename, summarise,\n#>     summarize\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(purrr)\n#> \n#> Attaching package: 'purrr'\n#> The following object is masked from 'package:plyr':\n#> \n#>     compact\nlibrary(tidyr)\n\n#Year with dplyr\nbaseball2 <- baseball %>% \n  group_by(id) %>% \n  mutate(cyear2 = year - min(year) + 1)\n\n#Models with dplyr (and others)\nbaseball3 = baseball2 %>% \n  filter(ab >= 25) %>% \n  group_by(id) %>% \n  nest() %>% \n  mutate(bmodels = map(data, model),\n         bcoefs = map_dfr(bmodels, function(x)(c(coef(x), rsquare = rsq(x))))) %>% \n  unnest(bcoefs) %>%  \n  mutate(intercept = `(Intercept)`, slope = cyear) %>% \n  select(-`(Intercept)`, -cyear) %>% \n  unnest(data) %>% \n  filter(rsquare > 0.999)\n\n#Verify that the years are the same\nall(baseball$cyear == baseball2$cyear2)\n#> [1] TRUE\n\n#Verify that the players are the same\nall(baseball3$id == results)\n#> [1] TRUE\n```\n:::\n\n    \nFor this blog post, I chose the baseball example to work with. Getting the number of years is fairly straightforward with dplyr using the `group_by` function and `mutate`. The main differences show up when modeling the data to each player. With plyr, it first creates a list of models and then applies some functions to that list. This is not possible with dplyr alone, but it is possible with the help of tidyr (for the `nest` and `unnest` functions) and purrr (for the `map` function). The results come out to be the same, but plyr uses less code.\n\n\n\n**2.  Which (base R) functions do you know that support the\n    split-apply-combine strategy? In your opinion, are these sufficient?\n    State why or why not?**\n\nThe base R functions that support the split-apply-combine strategy are the apply functions. I currently have found these apply functions to be sufficient and I often incorporate then whenever I can (even going as far to avoid for loops if I can help it). From the paper, it seems like the apply functions fall short when data takes more than two dimensions. They then require extra care to operate whereas the plyr functions extend the functionality of the apply functions more easily.\n    \n\n**3.  The ChatGPT response to the prompt \"Describe the split-apply-combine\n    paradigm\" is given below. Generally, the chatbot's answers are\n    decent. ChatGPT does have problems getting details right. Which\n    details would you change in the response?**\n\n> The \"split-apply-combine\" paradigm is a data analysis strategy used in\n> statistics and data science to analyze grouped data. It consists of\n> three steps:\n>\n> Split: The data is first split into groups based on one or more\n> variables of interest.\n>\n> Apply: A specific operation or function is applied to each group of\n> data, usually for the purpose of aggregating, summarizing, or\n> transforming the data within each group.\n>\n> Combine: The results of the operation applied to each group are then\n> combined and returned as a single output.\n>\n> The split-apply-combine paradigm is particularly useful in cases where\n> the data cannot be analyzed as a whole and instead must be analyzed in\n> smaller, more manageable chunks. This allows for more efficient data\n> processing and improved scalability. The paradigm is commonly\n> implemented using tools such as the \"groupby\" function in the R and\n> Python programming languages, and the \"dplyr\" library in R.\n\nI would have to say that ChatGPT is almost entirely correct, but the coding is a bit off. The `groupby` does not exist in R, and the tools necessary for the split-apply-combine paradigm need additional packages to use with dplyr. `groupby` is a simple fix by using `group_by` and the other useful packages are tidyr and purrr (along with plyr as another approach). In my experience of asking ChatGPT coding questions, it seems to combine different languages and/or syntaxes. Additionally, the apply step could use a little more expansion. This step could include more, such as using the data for some purpose such as modelling or graphing.\n\nA little off-topic, but another example of ChatGPT messing up a coding response is when I had it give me a mixed model using `aov()`, but used the random term syntax from `lme4`. It's close, but not quite there.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}