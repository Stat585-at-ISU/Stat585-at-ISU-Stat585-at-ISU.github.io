{
  "hash": "4304054b9a504bbe900e10f25683e652",
  "result": {
    "markdown": "---\nauthor: \"Charchit Shukla\"\ntitle: \"plyr and dplyr\"\ncategory: Split-apply-combine\ndate: 2022-02-16\noutput: github_document\n---\n\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-2_f281a2ed9bf69234a5e22f82736b6fd5'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(reshape)\n#> \n#> Attaching package: 'reshape'\n#> The following objects are masked from 'package:plyr':\n#> \n#>     rename, round_any\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:reshape':\n#> \n#>     rename\n#> The following objects are masked from 'package:plyr':\n#> \n#>     arrange, count, desc, failwith, id, mutate, rename, summarise,\n#>     summarize\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\ndata <- baseball\n\nrbi <- ddply(baseball, .(year), summarise, mean_rbi = mean(rbi, na.rm = TRUE))\nhead(rbi)\n#>   year mean_rbi\n#> 1 1871 22.28571\n#> 2 1872 20.53846\n#> 3 1873 30.92308\n#> 4 1874 29.00000\n#> 5 1875 31.58824\n#> 6 1876 30.13333\n\n#using dplyr package\n\nb2 <- group_by(data, year)\nrbi2 <- summarize(b2,mean(rbi))\nhead(rbi2)\n#> # A tibble: 6 Ã— 2\n#>    year `mean(rbi)`\n#>   <int>       <dbl>\n#> 1  1871        22.3\n#> 2  1872        20.5\n#> 3  1873        30.9\n#> 4  1874        29  \n#> 5  1875        31.6\n#> 6  1876        30.1\n```\n:::\n\n\n## Prompt:\n\nThe `plyr` package has by now been replaced by other, even faster\npackages, but the idea of *Split, apply, combine* is as relevant as\never.\n\nRead the paper [The Split-Apply-Combine Strategy for Data\nAnalysis](https://www.jstatsoft.org/article/view/v040i01) by Hadley\nWickham.\n\nWrite a blog post addressing the following questions:\n\n1.  The R code for the split-apply-combine paper is posted with the\n    paper. Pick one of the examples demonstrating `plyr` functionality\n    (such as `dlply` or `ddply`, ...) and rewrite the example using\n    functionality from the package `dplyr`. Make sure that your example\n    works and the results are identical.\n    \nCheck the code above.\n\n2.  Which (base R) functions do you know that support the\n    split-apply-combine strategy? In your opinion, are these sufficient?\n    State why or why not?\n    \nThe base R has functions like arrange, apply which can be combined with other functions like aggregate to use the split-combine strategy or even simply data$variable to select the variable. Other base function like split, subset, cbind, rbind etc. can also be used.\n\nThese base functions can get the job done for simpler datasets but the users have to go through inconsistent syntax to find the solution. However, it will be very hard working with higher dimensional data and different data structures as the paper mentioned.\n\n3. The ChatGPT response to the prompt \"Describe the split-apply-combine paradigm\" is given below. Generally, the chatbot's answers are decent. ChatGPT does have problems getting details right. Which details would you change in the response? \n\n    \n>    The \"split-apply-combine\" paradigm is a data analysis strategy used in statistics and data science to analyze grouped data. It consists of three steps:\n>\n>    Split: The data is first split into groups based on one or more variables of interest.\n>\n>    Apply: A specific operation or function is applied to each group of data, usually for the purpose of aggregating, summarizing, or transforming the data within each group.\n>\n> Combine: The results of the operation applied to each group are then combined and returned as a single output.\n>\n> The split-apply-combine paradigm is particularly useful in cases where the data cannot be analyzed as a whole and instead must be analyzed in smaller, more manageable chunks. This allows for more efficient data processing and improved scalability. The paradigm is commonly implemented using tools such as the \"groupby\" function in the R and Python programming languages, and the \"dplyr\" library in R.\n\nI think ChatGPT explained it well enough. The first thing which I caught was \"groupby\" function it has given as answer. Clearly, I have used above is \"group_by\" but ChatGPT mentioned it confidently in the quotation marks that it will be \"groupby\". In python as far as I know, the library is pandas. So, that does not seem right in the response above.\n\nI will be interested in hearing other students thoughts on this and what I mistakes I did not catch in the ChatGPT response.\n\n\nYou can write your answers directly the `README.Rmd` file. \nMake sure that the file knits (you will need to activate any packages your use in your code chunks with a call to `library(xxx)`, where xxx is the name of the package, such as `plyr` ).\nCommit your changes and push to your repo;  add any files in the `README_files` directory to your repository. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}